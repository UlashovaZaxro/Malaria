{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ffa1c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "data_dir = 'cell_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "577882f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load image paths and labels\n",
    "def load_image_paths_and_labels(data_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    categories = ['Uninfected', 'Parasitized']\n",
    "    label_map = {'Uninfected': 0, 'Parasitized': 1}\n",
    "    \n",
    "    for category in categories:\n",
    "        folder_path = os.path.join(data_dir, category)\n",
    "        for img_name in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            if img_name.endswith('.png'):\n",
    "                image_paths.append(img_path)\n",
    "                labels.append(label_map[category])\n",
    "    \n",
    "    return image_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07be310b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jami tasvirlar soni: 27552\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "image_paths, labels = load_image_paths_and_labels(data_dir)\n",
    "print(f'Jami tasvirlar soni: {len(image_paths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cca79acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trening to‘plami hajmi: 16530\n",
      "Validatsiya to‘plami hajmi: 5511\n",
      "Test to‘plami hajmi: 5511\n"
     ]
    }
   ],
   "source": [
    "# Split into train/validation/test (60% train, 20% validation, 20% test)\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    train_paths, train_labels, test_size=0.25, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "print(f'Trening to‘plami hajmi: {len(train_paths)}')\n",
    "print(f'Validatsiya to‘plami hajmi: {len(val_paths)}')\n",
    "print(f'Test to‘plami hajmi: {len(test_paths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c3e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class MalariaDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, target_size=(128, 128), augment=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.target_size = target_size\n",
    "        self.augment = augment\n",
    "        \n",
    "        # MalariaDataset __init__ metodidan (transformlar tarkibida):\n",
    "        self.base_transform = transforms.Compose([\n",
    "            transforms.ToTensor(), # [0, 255] oralig'idagi qiymatlarni [0, 1] ga o'tkazadi\n",
    "        ])\n",
    "        \n",
    "        if augment:\n",
    "            self.augment_transform = transforms.Compose([\n",
    "                transforms.RandomRotation(20),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomResizedCrop(target_size, scale=(0.8, 1.2)),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        # MalariaDataset __getitem__ metodidan:\n",
    "        img = cv2.resize(img, self.target_size)\n",
    "        \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  \n",
    "        \n",
    "        # Convert to PIL Image for torchvision transforms\n",
    "        from PIL import Image\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.augment:\n",
    "            img = self.augment_transform(img)\n",
    "        else:\n",
    "            img = self.base_transform(img)\n",
    "        \n",
    "        return img, torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e754dcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "batch_size = 32\n",
    "train_dataset = MalariaDataset(train_paths, train_labels, augment=True)\n",
    "val_dataset = MalariaDataset(val_paths, val_labels, augment=False)\n",
    "test_dataset = MalariaDataset(test_paths, test_labels, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97080c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 128),nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # CNNModel chiqish qatlami:\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "# self.model = nn.Sequential(\n",
    "#     # 3x [Conv2d -> (BatchNorm) -> ReLU -> MaxPool2d]\n",
    "#     # Flatten\n",
    "#     # Linear -> ReLU -> Dropout\n",
    "#     # Linear -> Sigmoid\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d6de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelni ishga tushirish\n",
    "model = CNNModel().to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "# Model keyinchalik o'qitiladi va bashorat qilish uchun ishlatiladi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efdde74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNNModel().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b49a4b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display sample images\n",
    "def plot_sample_images(image_paths, labels, num_samples=9):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    indices = np.random.choice(len(image_paths), num_samples, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        img = cv2.imread(image_paths[idx])\n",
    "        img = cv2.resize(img, (128, 128))\n",
    "        img = img / 255.0\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title('Parazitlanmagan' if labels[idx] == 0 else 'Parazitlangan')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle('Malariya Datasetidan Namuna Tasvirlar')\n",
    "    plt.savefig('sample_images.png')\n",
    "    plt.close()\n",
    "\n",
    "plot_sample_images(train_paths, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0305db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot feature maps\n",
    "def plot_feature_maps(model, image):\n",
    "    model.eval()\n",
    "    first_conv = model.model[0]  # First conv layer\n",
    "    image = image.unsqueeze(0).to(device)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        feature_maps = first_conv(image)\n",
    "    \n",
    "    feature_maps = feature_maps.cpu().numpy()\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(min(8, feature_maps.shape[1])):\n",
    "        plt.subplot(2, 4, i + 1)\n",
    "        plt.imshow(feature_maps[0, i, :, :], cmap='viridis')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle('Birinchi Konvolyutsion Qatlamdan Xususiyat Xaritalari')\n",
    "    plt.savefig('feature_maps.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62d668c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50):\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}: '\n",
    "            f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "            f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c026b70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss: 0.2832, Train Acc: 0.8936, Val Loss: 0.1524, Val Acc: 0.9521\n",
      "Epoch 2/50: Train Loss: 0.1683, Train Acc: 0.9495, Val Loss: 0.1450, Val Acc: 0.9566\n",
      "Epoch 3/50: Train Loss: 0.1511, Train Acc: 0.9517, Val Loss: 0.1279, Val Acc: 0.9595\n",
      "Epoch 4/50: Train Loss: 0.1465, Train Acc: 0.9518, Val Loss: 0.1188, Val Acc: 0.9617\n",
      "Epoch 5/50: Train Loss: 0.1412, Train Acc: 0.9544, Val Loss: 0.1230, Val Acc: 0.9588\n",
      "Epoch 6/50: Train Loss: 0.1361, Train Acc: 0.9555, Val Loss: 0.1272, Val Acc: 0.9597\n",
      "Epoch 7/50: Train Loss: 0.1334, Train Acc: 0.9552, Val Loss: 0.1371, Val Acc: 0.9532\n",
      "Epoch 8/50: Train Loss: 0.1352, Train Acc: 0.9558, Val Loss: 0.1187, Val Acc: 0.9617\n",
      "Epoch 9/50: Train Loss: 0.1347, Train Acc: 0.9580, Val Loss: 0.1245, Val Acc: 0.9594\n",
      "Epoch 10/50: Train Loss: 0.1309, Train Acc: 0.9554, Val Loss: 0.1236, Val Acc: 0.9612\n",
      "Epoch 11/50: Train Loss: 0.1333, Train Acc: 0.9566, Val Loss: 0.1226, Val Acc: 0.9595\n",
      "Epoch 12/50: Train Loss: 0.1341, Train Acc: 0.9573, Val Loss: 0.1230, Val Acc: 0.9606\n",
      "Epoch 13/50: Train Loss: 0.1247, Train Acc: 0.9593, Val Loss: 0.1220, Val Acc: 0.9601\n",
      "Epoch 14/50: Train Loss: 0.1229, Train Acc: 0.9577, Val Loss: 0.1187, Val Acc: 0.9614\n",
      "Epoch 15/50: Train Loss: 0.1199, Train Acc: 0.9590, Val Loss: 0.1311, Val Acc: 0.9568\n",
      "Epoch 16/50: Train Loss: 0.1213, Train Acc: 0.9583, Val Loss: 0.1148, Val Acc: 0.9612\n",
      "Epoch 17/50: Train Loss: 0.1253, Train Acc: 0.9601, Val Loss: 0.1222, Val Acc: 0.9615\n",
      "Epoch 18/50: Train Loss: 0.1202, Train Acc: 0.9586, Val Loss: 0.1323, Val Acc: 0.9566\n",
      "Epoch 19/50: Train Loss: 0.1170, Train Acc: 0.9621, Val Loss: 0.1173, Val Acc: 0.9626\n",
      "Epoch 20/50: Train Loss: 0.1194, Train Acc: 0.9593, Val Loss: 0.1229, Val Acc: 0.9615\n",
      "Epoch 21/50: Train Loss: 0.1169, Train Acc: 0.9607, Val Loss: 0.1104, Val Acc: 0.9652\n",
      "Epoch 22/50: Train Loss: 0.1102, Train Acc: 0.9613, Val Loss: 0.1090, Val Acc: 0.9639\n",
      "Epoch 23/50: Train Loss: 0.1111, Train Acc: 0.9624, Val Loss: 0.1122, Val Acc: 0.9633\n",
      "Epoch 24/50: Train Loss: 0.1137, Train Acc: 0.9621, Val Loss: 0.1064, Val Acc: 0.9668\n",
      "Epoch 25/50: Train Loss: 0.1149, Train Acc: 0.9623, Val Loss: 0.1071, Val Acc: 0.9664\n",
      "Epoch 26/50: Train Loss: 0.1111, Train Acc: 0.9633, Val Loss: 0.1171, Val Acc: 0.9615\n",
      "Epoch 27/50: Train Loss: 0.1084, Train Acc: 0.9620, Val Loss: 0.1167, Val Acc: 0.9623\n",
      "Epoch 28/50: Train Loss: 0.1075, Train Acc: 0.9644, Val Loss: 0.1103, Val Acc: 0.9648\n",
      "Epoch 29/50: Train Loss: 0.1068, Train Acc: 0.9629, Val Loss: 0.1198, Val Acc: 0.9614\n",
      "Epoch 30/50: Train Loss: 0.1156, Train Acc: 0.9630, Val Loss: 0.1059, Val Acc: 0.9662\n",
      "Epoch 31/50: Train Loss: 0.1062, Train Acc: 0.9642, Val Loss: 0.1112, Val Acc: 0.9650\n",
      "Epoch 32/50: Train Loss: 0.1058, Train Acc: 0.9636, Val Loss: 0.1130, Val Acc: 0.9655\n",
      "Epoch 33/50: Train Loss: 0.1026, Train Acc: 0.9643, Val Loss: 0.1098, Val Acc: 0.9677\n",
      "Epoch 34/50: Train Loss: 0.1062, Train Acc: 0.9643, Val Loss: 0.1056, Val Acc: 0.9664\n",
      "Epoch 35/50: Train Loss: 0.1020, Train Acc: 0.9655, Val Loss: 0.1061, Val Acc: 0.9661\n",
      "Epoch 36/50: Train Loss: 0.1023, Train Acc: 0.9667, Val Loss: 0.1356, Val Acc: 0.9592\n",
      "Epoch 37/50: Train Loss: 0.1049, Train Acc: 0.9649, Val Loss: 0.1406, Val Acc: 0.9545\n",
      "Epoch 38/50: Train Loss: 0.1014, Train Acc: 0.9646, Val Loss: 0.1132, Val Acc: 0.9623\n",
      "Epoch 39/50: Train Loss: 0.1005, Train Acc: 0.9655, Val Loss: 0.1131, Val Acc: 0.9653\n",
      "Epoch 40/50: Train Loss: 0.1086, Train Acc: 0.9647, Val Loss: 0.1012, Val Acc: 0.9670\n",
      "Epoch 41/50: Train Loss: 0.0994, Train Acc: 0.9661, Val Loss: 0.1154, Val Acc: 0.9633\n",
      "Epoch 42/50: Train Loss: 0.0985, Train Acc: 0.9650, Val Loss: 0.1214, Val Acc: 0.9610\n",
      "Epoch 43/50: Train Loss: 0.1021, Train Acc: 0.9685, Val Loss: 0.1087, Val Acc: 0.9626\n",
      "Epoch 44/50: Train Loss: 0.0968, Train Acc: 0.9675, Val Loss: 0.1034, Val Acc: 0.9677\n",
      "Epoch 45/50: Train Loss: 0.0988, Train Acc: 0.9669, Val Loss: 0.1140, Val Acc: 0.9643\n",
      "Epoch 46/50: Train Loss: 0.0975, Train Acc: 0.9675, Val Loss: 0.1121, Val Acc: 0.9617\n",
      "Epoch 47/50: Train Loss: 0.0979, Train Acc: 0.9673, Val Loss: 0.1089, Val Acc: 0.9655\n",
      "Epoch 48/50: Train Loss: 0.0984, Train Acc: 0.9660, Val Loss: 0.1176, Val Acc: 0.9601\n",
      "Epoch 49/50: Train Loss: 0.1019, Train Acc: 0.9672, Val Loss: 0.1092, Val Acc: 0.9668\n",
      "Epoch 50/50: Train Loss: 0.0993, Train Acc: 0.9698, Val Loss: 0.1046, Val Acc: 0.9662\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = train_model(model, train_loader, val_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68a91cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_acc'], label='Train Acc')\n",
    "    plt.plot(history['val_acc'], label='Val Acc')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcb5db07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test aniqligi: 0.9653, Test yo‘qotishi: 0.0969\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss, test_correct, test_total = 0.0, 0, 0\n",
    "    y_pred, y_true = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "            \n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc = test_correct / test_total\n",
    "    return test_loss, test_acc, y_true, y_pred\n",
    "\n",
    "test_loss, test_accuracy, y_true, y_pred = evaluate_model(model, test_loader)\n",
    "print(f'Test aniqligi: {test_accuracy:.4f}, Test yo‘qotishi: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aef013e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Parazitlanmagan', 'Parazitlangan'],\n",
    "            yticklabels=['Parazitlanmagan', 'Parazitlangan'])\n",
    "plt.title('Chalkashlik Matritsasi')\n",
    "plt.xlabel('Bashorat')\n",
    "plt.ylabel('Haqiqiy')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a433f545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasniflash Hisoboti:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Parazitlanmagan       0.96      0.97      0.97      2756\n",
      "  Parazitlangan       0.97      0.96      0.97      2755\n",
      "\n",
      "       accuracy                           0.97      5511\n",
      "      macro avg       0.97      0.97      0.97      5511\n",
      "   weighted avg       0.97      0.97      0.97      5511\n",
      "\n",
      "Precision: 0.9672\n",
      "Recall: 0.9633\n",
      "F1-Score: 0.9653\n"
     ]
    }
   ],
   "source": [
    "# Classification report and metrics\n",
    "print('Tasniflash Hisoboti:')\n",
    "print(classification_report(y_true, y_pred, target_names=['Parazitlanmagan', 'Parazitlangan']))\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "599ea58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature maps for a sample image\n",
    "sample_image, _ = train_dataset[0]\n",
    "plot_feature_maps(model, sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "936d9bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model state_dict saved to malaria_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' is your trained CNNModel instance\n",
    "MODEL_SAVE_PATH = 'malaria_model.pth' # Using .pth convention for state_dict\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"Model state_dict saved to {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99c65dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'malaria_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c910a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea1bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e2928d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
